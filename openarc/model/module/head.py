

# The BCHead (Basic Comprehension Head) serves as an initial diagnostic and analytical tool for 
# understanding the quality and characteristics of embeddings generated by a preceding OpenEmbedder.

import torch 
import torch.nn as nn
import torch.nn.functional as F

from openarc.config.config import config as C
from openarc.model.module.expert import _ExpertMLP, MultiScaleConvAggregator

# It operates in two main ways. Firstly, it performs an analysis by calculating the centroid (average embedding) for each unique token type present in a batch, 
# ignoring padding tokens. It then computes cosine similarities between these centroids to quantify the separability and relationships between different token 
# representations in the embedding space. This helps verify if distinct tokens (e.g., special markers vs. grid values) are being mapped to distinguishable regions. 

# Secondly, the head can include a simple linear reconstruction layer. This layer attempts to predict the original token ID from its corresponding embedding. 
# When evaluated with a loss function (like cross-entropy, ignoring padded positions), its performance (even before extensive training) provides a baseline measure 
# of how much information about the original tokens is preserved and decodable from the embeddings. A higher-than-random reconstruction accuracy, especially with 
# limited or no training, suggests that the combination of token and positional embeddings is already providing a somewhat structured and informative representation space.

class BCHead(nn.Module): 
    def __init__(self, config=C, enable_reconstruction_head: bool = True):
        super().__init__()
        self.config = config
        self.enable_reconstruction_head = enable_reconstruction_head

        if self.enable_reconstruction_head:
            self.reconstruction_layer = nn.Linear(config.hidden_size, config.vocab_size)

    def analyze_token_embeddings(self, embeddings: torch.Tensor, token_ids: torch.Tensor):
        """
        Analyzes embeddings to find centroids and similarities for different token types.
        Ignores padding tokens in analysis.
        """
        B, S, H = embeddings.shape
        analysis_results = {
            "mean_embeddings_per_token_type": {},
            "cosine_similarities": {}
        }

        # Flatten batch and sequence dimensions for easier processing,
        # but keep track of original token_ids to filter padding
        flat_embeddings = embeddings.view(B * S, H)
        flat_token_ids = token_ids.view(B * S)

        # Filter out padding tokens
        non_pad_mask = (flat_token_ids != self.config.pad)
        valid_embeddings = flat_embeddings[non_pad_mask]
        valid_token_ids = flat_token_ids[non_pad_mask]

        if valid_token_ids.numel() == 0: # All tokens were padding
            return analysis_results # Return empty results

        unique_present_token_ids = torch.unique(valid_token_ids)
        mean_embeddings_dict = {}

        for token_val in unique_present_token_ids:
            token_val_int = token_val.item()
            
            # Get all embeddings for this specific token ID
            token_specific_embeddings = valid_embeddings[valid_token_ids == token_val]
            if token_specific_embeddings.numel() > 0:
                mean_emb = token_specific_embeddings.mean(dim=0)
                mean_embeddings_dict[token_val_int] = mean_emb
                
                # TODO:
                # You could also store variance or other stats here
                # variance_emb = token_specific_embeddings.var(dim=0)
                # analysis_results["variance_embeddings_per_token_type"][token_val_int] = variance_emb

        analysis_results["mean_embeddings_per_token_type"] = mean_embeddings_dict

        # Calculate cosine similarities between mean embeddings
        token_keys = list(mean_embeddings_dict.keys())
        for i in range(len(token_keys)):
            for j in range(i + 1, len(token_keys)):
                key1, key2 = token_keys[i], token_keys[j]
                emb1 = mean_embeddings_dict[key1]
                emb2 = mean_embeddings_dict[key2]
                similarity = F.cosine_similarity(emb1.unsqueeze(0), emb2.unsqueeze(0)).item()
                analysis_results["cosine_similarities"][f"sim({key1}, {key2})"] = similarity

        return analysis_results

    def forward(self, embeddings: torch.Tensor, original_token_ids: torch.Tensor = None):
        """
        Args:
            embeddings (torch.Tensor): Output from OpenEmbedder (Batch, SeqLen, HiddenSize).
            original_token_ids (torch.Tensor, optional): Original token IDs (Batch, SeqLen).
                                                        Needed for analysis and reconstruction loss.
        Returns:
            dict: Containing 'analysis' and optionally 'reconstruction_logits'.
        """
        outputs = {}

        if original_token_ids is not None:
            analysis = self.analyze_token_embeddings(embeddings, original_token_ids)
            outputs["analysis"] = analysis
        else:
            outputs["analysis"] = "original_token_ids not provided for analysis."

        if self.enable_reconstruction_head:
            reconstruction_logits = self.reconstruction_layer(embeddings)
            outputs["reconstruction_logits"] = reconstruction_logits
        
        return outputs
    

class EHead(nn.Module):
    def __init__(self, dim: int, mlp_intermediate_size: int, mlp_activation_str: str = 'silu', dropout_rate: float = 0.1):
        super().__init__()
        self.dim = dim

        if mlp_activation_str.lower() == "silu": activation_fn = nn.SiLU()
        elif mlp_activation_str.lower() == "gelu": activation_fn = nn.GELU()
        elif mlp_activation_str.lower() == "relu": activation_fn = nn.ReLU()
        else: raise ValueError(f"Unsupported activation: {mlp_activation_str}")

        self.mlp = _ExpertMLP(
            hidden_size=self.dim,
            intermediate_size=mlp_intermediate_size,
            activation_fn=activation_fn,
            dropout_rate=dropout_rate # Pass the EHead's dropout to the MLP
        )
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        if x.shape[-1] != self.dim:
            raise ValueError(
                f"Input feature dimension {x.shape[-1]} "
                f"does not match EHead's dim {self.dim}"
            )
        
        output = self.mlp(x) 
        return output
    
class TEHead(nn.Module):
    def __init__(self, dim: int, mlp_config, dropout_rate: float = 0.1, num_layer: int = 2, ehead_dropout_rate: float = 0.1): # Changed rc_head_dropout_rate
        super().__init__()
        self.dim = dim # Input per-token dim from sequence (e.g., hidden_size of Transformer)
        self.num_layer = num_layer

        self.internal_feature_dim_after_aggregator = C.intermediate_size # Let's assume this is the target for the aggregator

        self.bc_lm_aggregator = MultiScaleConvAggregator( # Renamed for clarity
            in_channels=self.dim, # Takes hidden_size from sequence
            target_output_channels=self.internal_feature_dim_after_aggregator,
            # branch_channels_ratio=0.25, # Can be part of mlp_config or hardcoded
            # min_branch_channels=max(16, self.internal_feature_dim_after_aggregator // 8),
            kernel_sizes=C.moe_feature_extractor_kernel_sizes
        )
        
        self.multi_e_head = nn.ModuleList() # Renamed for clarity
        for _ in range(self.num_layer):
            # EHead now takes mlp_intermediate_size and mlp_activation_str
            # Its 'dim' is self.internal_feature_dim_after_aggregator
            self.multi_e_head.append(
                EHead(
                    dim=self.internal_feature_dim_after_aggregator,
                    mlp_intermediate_size=C.intermediate_size, # Or a different value for EHead's own MLP
                    mlp_activation_str=getattr(mlp_config, 'mlp_activation', 'silu'),
                    dropout_rate=ehead_dropout_rate
                )
            )
        
        self.dropout = nn.Dropout(dropout_rate)
        # This projects the output of EHeads back to a desired final dimension (e.g. num_classes, or original dim)
        self.bc_end_projection = nn.Linear(self.internal_feature_dim_after_aggregator, self.dim) # Renamed

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # x: (batch_size, sequence_length, self.dim)
        if x.ndim != 3 or x.shape[-1] != self.dim:
            raise ValueError(f"Input shape error. Expected (B, S, {self.dim}), Got {x.shape}")
        
        x_permuted = x.permute(0, 2, 1) # (B, self.dim, S)
        aggregated_features = self.bc_lm_aggregator(x_permuted) # (B, self.internal_feature_dim_after_aggregator)
        
        refined_features = aggregated_features
        for e_head_layer in self.multi_e_head:
            refined_features = e_head_layer(refined_features)
        
        output_after_dropout = self.dropout(refined_features)
        final_output = self.bc_end_projection(output_after_dropout) # (B, self.dim)
        
        return final_output
